{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T00:01:48.425231Z",
     "start_time": "2021-06-03T00:01:48.414825Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly_express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import init_notebook_mode\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "# setting default template to plotly_white for all visualizations\n",
    "pio.templates.default = \"plotly_white\"\n",
    "%matplotlib inline\n",
    "import gc\n",
    "\n",
    "import utils_eda as uteda\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image level csv shape : (6334, 4)\n",
      "Train study level csv shape : (6054, 5)\n"
     ]
    }
   ],
   "source": [
    "PATH = './input/siim-covid19-detection'\n",
    "submission = pd.read_csv(os.path.join(PATH,'sample_submission.csv'), index_col=None)\n",
    "image_df = pd.read_csv(os.path.join(PATH,'train_image_level.csv'), index_col=None)\n",
    "study_df = pd.read_csv(os.path.join(PATH,'train_study_level.csv'), index_col=None)\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(f\"Train image level csv shape : {image_df.shape}\\nTrain study level csv shape : {study_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6334"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_df.head(2)\n",
    "len(image_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6054"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_df.head(2)\n",
    "len(study_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number train dcms=6334, number test dcms from test dir=1263,\n",
      "sample_submission.csv has 2477 entries, this includes 1263 images + 1214 studies \n"
     ]
    }
   ],
   "source": [
    "#get a list of all the files\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "all_files = []\n",
    "trn_files = uteda.get_files(PATH+'/train')\n",
    "test_files = uteda.get_files(PATH+'/test')\n",
    "\n",
    "print(f'number train dcms={len(trn_files)}, number test dcms from test dir={len(test_files)},\\nsample_submission.csv has {len(submission)} entries, this includes 1263 images + 1214 studies ')\n",
    "all_files = trn_files+test_files\n",
    "# ds=dcmread(all_files[0])\n",
    "# dir(ds)\n",
    "# ds.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getinfo(ds,col='id'):\n",
    "    #lets see if the ds file has only studies\n",
    "    #and what the mix is\n",
    "    #assumme we are looking at id column\n",
    "    out = ds[col].map(lambda x:x.split('_')[1])\n",
    "    print (f'total records={out.shape}')\n",
    "    print(f'unique vals={set(out)}')\n",
    "    for val in (set(out)):\n",
    "        f = lambda x:x==val\n",
    "        tot = sum(map(f,out))\n",
    "        print(f'sum {val} = {tot}')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total records=(6334,)\n",
      "unique vals={'image'}\n",
      "sum image = 6334\n",
      "\n",
      "\n",
      "total records=(6054,)\n",
      "unique vals={'study'}\n",
      "sum study = 6054\n",
      "\n",
      "\n",
      "total records=(2477,)\n",
      "unique vals={'image', 'study'}\n",
      "sum image = 1263\n",
      "sum study = 1214\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getinfo(image_df)\n",
    "getinfo(study_df)\n",
    "getinfo(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_from_kaggle_sub## Where are submission files? In test dir!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of _image and _study\n",
    "image_df['id']=image_df['id'].map(lambda x: x.split('_')[0])\n",
    "study_df['id']=study_df['id'].map(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the bounding box distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets find max number bounding boxes in train set bounding boxes\n",
    "f=lambda x:x.count('{') if type(x) is str else 0\n",
    "bbox_distribution=list(map(f,image_df.boxes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the bounding box distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of bounding boxes=8\n",
      "\n",
      "number images with 0 bounding boxes=2040\n",
      "number images with 1 bounding boxes=973\n",
      "number images with 2 bounding boxes=3113\n",
      "number images with 3 bounding boxes=183\n",
      "number images with 4 bounding boxes=23\n",
      "number images with 5 bounding boxes=1\n",
      "number images with 6 bounding boxes=0\n",
      "number images with 7 bounding boxes=0\n",
      "number images with 8 bounding boxes=1\n",
      "\n",
      "Total images=6334, total with bounding boxes=4294\n",
      "\n",
      "0.32 % has 0 bounding boxes\n",
      "0.15 % has 1 bounding boxes\n",
      "0.49 % has 2 bounding boxes\n",
      "0.03 % has 3 bounding boxes\n",
      "0.00 % has 4 bounding boxes\n",
      "0.00 % has 5 bounding boxes\n",
      "0.00 % has 6 bounding boxes\n",
      "0.00 % has 7 bounding boxes\n",
      "0.00 % has 8 bounding boxes\n"
     ]
    }
   ],
   "source": [
    "m=max(bbox_distribution)\n",
    "print(f'Maximum number of bounding boxes={m}\\n')\n",
    "tots=0\n",
    "\n",
    "percentages=[]\n",
    "for i in range(0, m+1):\n",
    "    tot = bbox_distribution.count(i)\n",
    "    print(f'number images with {i} bounding boxes={tot}')\n",
    "    percentages.append(tot)\n",
    "    tots+=tot\n",
    "print(f'\\nTotal images={tots}, total with bounding boxes={tots-bbox_distribution.count(0)}\\n')\n",
    "\n",
    "percentages=[x/tots for x in percentages]\n",
    "for i,pct in enumerate(percentages):\n",
    "    print(f'{\"{:.2f}\".format(pct)} % has {i} bounding boxes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframe with rows of interest, 3 ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROWS_WITH_THIS_MANY_BBOXES=10\n",
    "NUMBER_OF_BOXES=5  \n",
    "DO_TINY_DATASET_ON_OVERTRAINED_MODEL=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. dataframe that ONLY contains images that have NUMB_BOXES bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows_with_numb_boxes(df, numb_bboxes_wanted):\n",
    "    '''\n",
    "     find dcm files with numb_boxes bounding boxes\n",
    "     works by finding number of dicts by checking for the first { char in the dict\n",
    "     param:  df- loaded from train_image_level.csv (there are some images with no bounding boxes\n",
    "     so the boxes field is blank)\n",
    "             numb_bboxes_wanted-return df with rows with this many bounding boxes \n",
    "     returns: dataframe\n",
    "    '''\n",
    "    if 'numb_boxes' not in df.columns:\n",
    "        f=lambda x:x.count('{') if type(x) is str else 0\n",
    "        df['numb_boxes'] = list(map(f,df.boxes))\n",
    "    \n",
    "    mask= map(lambda x:x==numb_bboxes_wanted, df['numb_boxes'])\n",
    "    df1=df[list(mask)]\n",
    "    df1.reset_index(inplace=True, drop=True)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe contains 1 rows with 5 bounding boxes\n"
     ]
    }
   ],
   "source": [
    "df1=get_rows_with_numb_boxes(image_df,NUMBER_OF_BOXES)\n",
    "# df1=get_rows(image_df,8)#max bounding boxes, just 1 of these\n",
    "print(f'Dataframe contains { len(df1)} rows with {NUMBER_OF_BOXES} bounding boxes')\n",
    "# df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. dataframe with even distribution of bounding boxes, up to MAX_ROWS_WITH_THIS_MANY_BBOXES each) (not representative of the above bbounding box distribution though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe contains 52 rows with 5 bounding boxes\n",
      "CPU times: user 15.4 ms, sys: 0 ns, total: 15.4 ms\n",
      "Wall time: 14.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df1=pd.DataFrame()\n",
    "for i in range((max(bbox_distribution)+1)):\n",
    "    df1=df1.append(get_rows_with_numb_boxes(image_df,i)[:MAX_ROWS_WITH_THIS_MANY_BBOXES])\n",
    "df1.reset_index(inplace=True, drop=True)\n",
    "print(f'Dataframe contains { len(df1)} rows with {NUMBER_OF_BOXES} bounding boxes')\n",
    "# df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. something totally different, build a tiny dataset, use it to verify model can be overfit from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not doing tiny dataset\n"
     ]
    }
   ],
   "source": [
    "if DO_TINY_DATASET_ON_OVERTRAINED_MODEL==True:\n",
    "    #create the tiny dataset\n",
    "    TINY_PATH='tmp/covid_small/images/train'\n",
    "    fls = uteda.get_files(TINY_PATH)\n",
    "    f=lambda x:x.split('/')[-1]\n",
    "    fls = [f(fle) for fle in fls]\n",
    "    fls = [(lambda x: x.split('.')[0])(x) for x in fls]\n",
    "    # fls\n",
    "\n",
    "    %%time\n",
    "    df1=image_df[image_df['id'].isin(fls)]\n",
    "\n",
    "    # Load meta.csv file\n",
    "    # Original dimensions are required to scale the bounding box coordinates appropriately.\n",
    "    meta_df = pd.read_csv('input/siim-covid19-resized-to-512px-png/meta.csv')\n",
    "\n",
    "    cols=meta_df.columns\n",
    "    cols=[col for col in cols]\n",
    "    cols[0]='id'\n",
    "    meta_df.columns=cols\n",
    "    len(meta_df)\n",
    "\n",
    "    # Merge both the dataframes\n",
    "    df1 = df1.merge(meta_df, on='id',how=\"left\")\n",
    "    df1.head(2)\n",
    "    df1\n",
    "else:\n",
    "    print(\"Not doing tiny dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_eda as uteda\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "def copy_dicom_img_to_dir(row, pth_dicom_fles, pth_destdir):\n",
    "    '''\n",
    "    row - pandas series\n",
    "    pth_dicom_fles - 'input/siim-covid19-detection/train/' for ex\n",
    "    pth_destdir - where all images will wind up (like ''./test_tmp/'')\n",
    "    return im.shape(height,width) to be logged\n",
    "    \n",
    "    ex.\n",
    "    # copy imag to test dir\n",
    "    for i in range(MAX_ROWS):\n",
    "         copy_dicom_img_to_dir(df1.loc[i],TRAIN_DIR ,TEST_DIR )\n",
    "    \n",
    "    '''       \n",
    "    study=  row.loc['StudyInstanceUID']\n",
    "    dcm_file=row.loc['id']\n",
    "    \n",
    "    #create a path to the study\n",
    "    pth =  pth_dicom_fles + row.loc['StudyInstanceUID']\n",
    "\n",
    "    #get all dicom files from the study\n",
    "    dcms = uteda.get_files(pth) \n",
    "\n",
    "    if (len(dcms)>1):\n",
    "        #find the correct image\n",
    "        dcms=list(filter(lambda x:dcm_file in x, dcms))\n",
    "   \n",
    "    #get the image\n",
    "    img=uteda.dicom2array(dcms[0])\n",
    "    \n",
    "    #save it to path\n",
    "    if not os.path.exists(pth_destdir):\n",
    "        os.mkdir(pth_destdir)\n",
    "\n",
    "    im = Image.fromarray(img)     \n",
    "    im.save(pth_destdir+dcm_file+'.png')\n",
    "    return img.shape\n",
    "\n",
    "def load_img( pth_destdir, imagename):\n",
    "    '''\n",
    "    just loads an image\n",
    "    ex.\n",
    "    nme = df1.loc[0,'id']+'.png'\n",
    "    im= load_img( TEST_DIR, nme)\n",
    "    '''\n",
    "    return Image.open(pth_destdir + imagename)\n",
    "    \n",
    "def get_boxes(row):\n",
    "    '''\n",
    "    Convert the string that contaings bounding boxes \n",
    "    into a list of dicts and return\n",
    "    ex.\n",
    "    # get boxes\n",
    "    all_boxes=[]\n",
    "    for i in range(MAX_ROWS):\n",
    "        all_boxes.append(get_boxes(df1.loc[i]))\n",
    "    '''\n",
    "\n",
    "    if (pd.isnull(row.loc['boxes'])):\n",
    "        return []\n",
    "    \n",
    "    boxes=row.loc['boxes'].replace('\\'','\"')\n",
    "    return json.loads(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine maximum predictions to make from df1 result set\n",
    "## Copy images to test dir and get image bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_ROWS=len(df1)\n",
    "MAX_ROWS\n",
    "DO_TINY_DATASET_ON_OVERTRAINED_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3141eaf97a8943cfb4a919a7a22a9abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81d912cd5d341d99b50c6d87a1c438b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-47bfccae3982>\u001b[0m in \u001b[0;36mcopy_dicom_img_to_dir\u001b[0;34m(row, pth_dicom_fles, pth_destdir)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpth_destdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdcm_file\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2172\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2173\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m             \u001b[0;31m# do what we can to clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0m_write_multiple_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m         \u001b[0mImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_idat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m                     \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m                     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if DO_TINY_DATASET_ON_OVERTRAINED_MODEL == True:\n",
    "    print(\"Doing tiny dataset\")\n",
    "else:\n",
    "    # NOTE: DO NOT run this if you are doing the tiny dataset route above, it already has files elsewhere to use \n",
    "    #lets get the boxes for the first MAX_ROWS rows of df1 and save the images associated with the dicom files\n",
    "    import json\n",
    "    TRAIN_DIR='./input/siim-covid19-detection/train/'\n",
    "    TEST_DIR='./test_tmp/'\n",
    "\n",
    "    # get boxes\n",
    "    all_boxes=[]\n",
    "    for i in tqdm(range(MAX_ROWS)):\n",
    "        all_boxes.append(get_boxes(df1.loc[i]))\n",
    "\n",
    "    # clear dest dir first\n",
    "    if os.path.exists(TEST_DIR):\n",
    "        for file in os.scandir(TEST_DIR):\n",
    "            if os.path.isfile(file):\n",
    "                os.remove(file.path)\n",
    "\n",
    "    # copy imag to test dir\n",
    "    for i in tqdm(range(MAX_ROWS)):\n",
    "         copy_dicom_img_to_dir(df1.loc[i],TRAIN_DIR ,TEST_DIR )\n",
    "\n",
    "    # print(all_boxes)   \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a trained yolov5 model and run predictions on files in TEST_DIR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/best_exp11_yolov5l_img512_1class.pt\n",
      "/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/best_exp10_yolov5s_img256_1class.pt\n",
      "/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/best_exp12_yolov5m_img512_1class.pt\n",
      "/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/best_from_kaggle_sub_1class.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "\n",
    "# MODEL1_PATH_2CLASSES ='/artifacts/run_3bh5hck7_model:v199/best.pt'#use alone\n",
    "MODEL_PATH_1CLASS=cwd +\"/best_exp11_yolov5l_img512_1class.pt\" #can ensemble with below\n",
    "MODEL_PATH_2CLASS=cwd +\"/best_exp10_yolov5s_img256_1class.pt\" #can ensemble with below\n",
    "MODEL_PATH_3CLASS=cwd +\"/best_exp12_yolov5m_img512_1class.pt\" #can ensemble with below\n",
    "MODEL_PATH_4CLASS_KAGGLE = cwd +'/best_from_kaggle_sub_1class.pt'\n",
    "\n",
    "#overfit to prove model viability and that image prediction code is correct\n",
    "MODEL_OVERFIT_ON_TINY_DATA=cwd+\"/tmp/yolov5/kaggle-siim-covid_tiny/exp7/weights/best.pt\"\n",
    "TEST_DIR=cwd+'/test_tmp/'\n",
    "\n",
    "#used for debugging yolov5/detect.py in seperate pycharm session\n",
    "print(MODEL_PATH_1CLASS)\n",
    "print(MODEL_PATH_2CLASS)\n",
    "print(MODEL_PATH_3CLASS)\n",
    "print(MODEL_PATH_4CLASS_KAGGLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run models, or ensembles of models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MODEL_PATH_1CLASS': '/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/best_exp11_yolov5l_img512_1class.pt',\n",
       " 'MODEL_PATH_2CLASS': '/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/best_exp10_yolov5s_img256_1class.pt',\n",
       " 'MODEL_PATH_3CLASS': '/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/best_exp12_yolov5m_img512_1class.pt',\n",
       " 'MODEL_PATH_4CLASS_KAGGLE': '/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/best_from_kaggle_sub_1class.pt'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if(DO_TINY_DATASET_ON_OVERTRAINED_MODEL == True):\n",
    "    #NOTE: use the following 3 to test on overtrained model, otherwise \n",
    "    # comment all 3 out and uncomment above\n",
    "    run_names=['MODEL_OVERFIT_ON_TINY_DATA']\n",
    "    model_names=[ MODEL_OVERFIT_ON_TINY_DATA]\n",
    "    TEST_DIR=cwd+'/tmp/covid_small/images/train/'\n",
    "else:       \n",
    "    # keys= name of the predict output dir\n",
    "    # vals=model weights we are using per run \n",
    "    run_names=['MODEL_PATH_1CLASS','MODEL_PATH_2CLASS','MODEL_PATH_3CLASS','MODEL_PATH_4CLASS_KAGGLE']\n",
    "    model_names=[ MODEL_PATH_1CLASS, MODEL_PATH_2CLASS,MODEL_PATH_3CLASS, MODEL_PATH_4CLASS_KAGGLE]\n",
    "\n",
    "# model_names=[ MODEL_PATH_1CLASS + \" \"+MODEL_PATH_2CLASS, MODEL_PATH_1CLASS,MODEL_PATH_2CLASS, MODEL_PATH_3CLASS_KAGGLE]\n",
    "model_sel=dict(zip(run_names,model_names))\n",
    "model_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/yolov5\n",
      "YOLOv5 🚀 v5.0-303-g3bef77f torch 1.9.0+cu102 CUDA:0 (TITAN Xp, 12194.0625MB)\n",
      "\n",
      "Fusing layers... \n",
      "/home/keith/anaconda3/envs/p39/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "Model Summary: 392 layers, 46600566 parameters, 0 gradients, 114.1 GFLOPs\n",
      "libpng error: Read Error\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/yolov5/detect.py\", line 230, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/yolov5/detect.py\", line 225, in main\n",
      "    run(**vars(opt))\n",
      "  File \"/home/keith/anaconda3/envs/p39/lib/python3.9/site-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/yolov5/detect.py\", line 95, in run\n",
      "    for path, img, im0s, vid_cap in dataset:\n",
      "  File \"/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/yolov5/utils/datasets.py\", line 213, in __next__\n",
      "    assert img0 is not None, 'Image Not Found ' + path\n",
      "AssertionError: Image Not Found /home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/test_tmp/9178cdb575d7.png\n",
      "YOLOv5 🚀 v5.0-303-g3bef77f torch 1.9.0+cu102 CUDA:0 (TITAN Xp, 12194.0625MB)\n",
      "\n",
      "Fusing layers... \n",
      "/home/keith/anaconda3/envs/p39/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "Model Summary: 224 layers, 7053910 parameters, 0 gradients, 16.3 GFLOPs\n",
      "libpng error: Read Error\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/yolov5/detect.py\", line 230, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/yolov5/detect.py\", line 225, in main\n",
      "    run(**vars(opt))\n",
      "  File \"/home/keith/anaconda3/envs/p39/lib/python3.9/site-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/yolov5/detect.py\", line 95, in run\n",
      "    for path, img, im0s, vid_cap in dataset:\n",
      "  File \"/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/yolov5/utils/datasets.py\", line 213, in __next__\n",
      "    assert img0 is not None, 'Image Not Found ' + path\n",
      "AssertionError: Image Not Found /home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/test_tmp/9178cdb575d7.png\n",
      "YOLOv5 🚀 v5.0-303-g3bef77f torch 1.9.0+cu102 CUDA:0 (TITAN Xp, 12194.0625MB)\n",
      "\n",
      "Fusing layers... \n",
      "/home/keith/anaconda3/envs/p39/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "Model Summary: 308 layers, 21037638 parameters, 0 gradients, 50.3 GFLOPs\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/yolov5/detect.py\", line 230, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/yolov5/detect.py\", line 225, in main\n",
      "    run(**vars(opt))\n",
      "  File \"/home/keith/anaconda3/envs/p39/lib/python3.9/site-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/yolov5/detect.py\", line 166, in run\n",
      "    cv2.imwrite(save_path, im0)\n",
      "KeyboardInterrupt\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection/yolov5/detect.py\", line 13, in <module>\n",
      "    import torch\n",
      "  File \"/home/keith/anaconda3/envs/p39/lib/python3.9/site-packages/torch/__init__.py\", line 695, in <module>\n",
      "    from torch import hub as hub\n",
      "  File \"/home/keith/anaconda3/envs/p39/lib/python3.9/site-packages/torch/hub.py\", line 17, in <module>\n",
      "    from tqdm.auto import tqdm  # automatically select proper tqdm submodule if available\n",
      "  File \"/home/keith/anaconda3/envs/p39/lib/python3.9/site-packages/tqdm/__init__.py\", line 3, in <module>\n",
      "    from .cli import main  # TODO: remove in v5.0.0\n",
      "  File \"/home/keith/anaconda3/envs/p39/lib/python3.9/site-packages/tqdm/cli.py\", line 9, in <module>\n",
      "    from .std import TqdmKeyError, TqdmTypeError, tqdm\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 851, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 983, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 647, in _compile_bytecode\n",
      "KeyboardInterrupt\n",
      "/home/keith/AA_jupyter_tuts/kaggle_SIIM_COVID_Detection\n",
      "CPU times: user 394 ms, sys: 129 ms, total: 523 ms\n",
      "Wall time: 56.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%cd tmp/yolov5\n",
    "\n",
    "for key, val in model_sel.items():   \n",
    "    %rm -rf ./runs/detect/{key}\n",
    "#     params=\"--weights \" + val+\" --source \" + TEST_DIR+ \"  --img 512 --conf 0.281 --iou-thres 0.5 --max-det 8  --save-txt  --save-conf --name \" + key + \" --exist-ok\"\n",
    "#CHANGING CONFIDENCE SCORE\n",
    "    params=\"--weights \" + val+\" --source \" + TEST_DIR+ \"  --img 512 --conf 0.081 --iou-thres 0.5 --max-det 4  --save-txt  --save-conf --name \" + key + \" --exist-ok\"\n",
    "\n",
    "    !python detect.py {params} >out.txt\n",
    "# #get back to correct dir\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels saved to runs/detect/{model_sel key}/labels, 1st is class, last is confidence.  Looks like following\n",
    "1 0.326869 0.343108 0.252468 0.463613 0.402857 <br>\n",
    "1 0.695874 0.373716 0.246474 0.478596 0.450281\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #list images we tested on\n",
    "# all_files = []\n",
    "# for dirname,_,filenames in os.walk(TEST_DIR):\n",
    "#     for filename in filenames:\n",
    "#         all_files.append(os.path.join(dirname, filename))\n",
    "# print(all_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get an image, the ground truth bounding boxes, and 1 or more predicted bounding boxes and display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📍 Note: 1 is class id (opacity), the first four float numbers are `x_center`, `y_center`, `width` and `height`. The final float value is `confidence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PRED_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-df969faf8c14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPRED_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of test images predicted as opaque: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PRED_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "prediction_files = os.listdir(PRED_PATH)\n",
    "print('Number of test images predicted as opaque: ', len(prediction_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📍 Out of 1263 test images, 583 were predicted with `opacity` label and thus we have that many prediction txt files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"https://www.kaggle.com/embed/rajsengo/beginner-eda-siim-covid-19-detection?cellId=14&cellIds=13&kernelSessionId=64636915\" height=\"300\" style=\"margin: 0 auto; width: 100%; max-width: 950px;\" frameborder=\"0\" scrolling=\"auto\" title=\"[Beginner EDA] SIIM COVID-19 Detection\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR_PREDS='./yolov5/runs/detect/'\n",
    "\n",
    "# The submisison requires xmin, ymin, xmax, ymax format. \n",
    "# YOLOv5 returns x_center, y_center, width, height\n",
    "def correct_bbox_format(bboxes, orig_width, orig_height):\n",
    "    correct_bboxes = []\n",
    "    for b in bboxes:\n",
    "        xc, yc = int(np.round(b[1]*orig_width)), int(np.round(b[2]*orig_height))\n",
    "        w, h = int(np.round(b[3]*orig_width)), int(np.round(b[4]*orig_height))\n",
    "\n",
    "        xmin= xc - int(np.round(w/2))\n",
    "        ymin= yc - int(np.round(h/2))\n",
    "        xmax= xc + int(np.round(w/2))\n",
    "        ymax= yc + int(np.round(h/2))\n",
    "        conf= b[5]\n",
    "        \n",
    "        correct_bboxes.append([xmin, ymin, xmax, ymax, conf])\n",
    "        \n",
    "    return correct_bboxes\n",
    "\n",
    "def get_pred_bboxes(img_name, run_names, run_dir,orig_width=None, orig_height=None):\n",
    "    '''\n",
    "    img_name name + suffix\n",
    "    get all the bounding boxes for img that are stored in multiple run directories\n",
    "    '''\n",
    "    if(orig_width is None or orig_height is None):\n",
    "        im= load_img( TEST_DIR, img_name)\n",
    "        im=np.array(im)\n",
    "        orig_height,orig_width = im.shape\n",
    "    \n",
    "    results=[]\n",
    "    for dir in run_names:\n",
    "        #convert bounding boxes into lists of floats\n",
    "        pred_boxes_and_confidence=[]\n",
    "        \n",
    "        #file to open\n",
    "        fle = OUT_DIR_PREDS+dir+ '/labels/' + img_name.split('.')[0] +'.txt'\n",
    "        if not os.path.isfile(fle):\n",
    "            print(f'Missing label file for image {img_name} for run {dir}' )\n",
    "            results.append([])\n",
    "            continue\n",
    "            \n",
    "        with open(fle) as f:\n",
    "            lines=f.readlines()\n",
    "            for lne in lines:\n",
    "                lne=lne.replace('\\n','')\n",
    "                lne=\"[\" +lne.replace(' ',',') +\"]\"\n",
    "                lne=json.loads(lne)               \n",
    "                pred_boxes_and_confidence.append(lne)\n",
    "        pred_boxes = correct_bbox_format(pred_boxes_and_confidence,orig_width,orig_height)\n",
    "        \n",
    "        #convert to a dict\n",
    "        keys=[\"x1\",\"y1\",\"x2\",\"y2\",\"conf\"]\n",
    "        for i,b in enumerate(pred_boxes):\n",
    "            pred_boxes[i]=dict(zip(keys,b))\n",
    "        results.append(pred_boxes)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tying it together, show images, ground truth and all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the bounding boxes according to the size of the resized image. \n",
    "def scale_bbox(row, bboxes, img_x, img_y):\n",
    "    # Get scaling factor\n",
    "    scale_x = img_x/row.dim1\n",
    "    scale_y = img_y/row.dim0\n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        bbox['x'] = int(np.round(bbox['x']*scale_x, 4))\n",
    "        bbox['y'] = int(np.round(bbox['y']*scale_y, 4))\n",
    "        bbox['width'] = int(np.round(bbox['width']*(scale_x), 4))\n",
    "        bbox['height']= int(np.round(bbox['height']*scale_y, 4))\n",
    "        \n",
    "    return bboxes\n",
    "\n",
    "def generate_images(df,max_rows, test_dir,out_dir, run_names, show_image, img_suffix='.png'):\n",
    "#     print(out_dir)\n",
    "    for i in tqdm(range(max_rows)):\n",
    "        row=df.loc[i]\n",
    "\n",
    "        img_name = row.loc['id']+img_suffix\n",
    "\n",
    "        img= load_img( test_dir, img_name)\n",
    "        img=np.array(img)\n",
    "        height,width = img.shape\n",
    "#         print(f'Height={height} Width={width}')     \n",
    "\n",
    "        #ground truth bounding boxes\n",
    "        gt_boxes=get_boxes(row)\n",
    "#         print(gt_boxes)\n",
    "\n",
    "        #lets see if df1 has better h,w info\n",
    "        if 'dim0' in row:\n",
    "            gt_boxes = scale_bbox(row, gt_boxes, width, height)\n",
    "              \n",
    "        #get predicted bounding boxes\n",
    "        results = get_pred_bboxes(img_name, run_names, out_dir, orig_width=width, orig_height=height)\n",
    "#         print(results)\n",
    "        \n",
    "        results1=dict(zip(run_names,results)) \n",
    "\n",
    "        #plot it with the b_boxes\n",
    "        uteda.plot_img_with_bboxes(img,img_name, gt_boxes,results1, size=15, out_path=out_dir,show_image=show_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_IMAGES_DIR='output_'+str([*model_sel][0])+'_gt_bboxes/'\n",
    "OUTPUT_IMAGES_DIR\n",
    "# TEST_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_suffix= '.jpg' if DO_TINY_DATASET_ON_OVERTRAINED_MODEL == True else '.png'\n",
    "img_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see a single image\n",
    "generate_images(df1, 1, TEST_DIR,OUTPUT_IMAGES_DIR, run_names=run_names, show_image=True, img_suffix=img_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see a single image\n",
    "generate_images(df1, 1, TEST_DIR,OUTPUT_IMAGES_DIR, run_names=run_names, show_image=True, img_suffix=img_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all marked up images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_IMAGES_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# clear dest dir first\n",
    "if os.path.exists(OUTPUT_IMAGES_DIR):\n",
    "    for file in os.scandir(OUTPUT_IMAGES_DIR):\n",
    "        if os.path.isfile(file):\n",
    "            os.remove(file.path)\n",
    "\n",
    "#now lets generate images for all the rows we predicted for later display\n",
    "generate_images(df1, MAX_ROWS, TEST_DIR,OUTPUT_IMAGES_DIR, run_names=run_names, show_image=False, img_suffix=img_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display bunch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters what we see, set to None to see all\n",
    "show_only_with_this_many_GT_boxes=0\n",
    "\n",
    "# get a list of files of interest\n",
    "fls=df1.loc[df1['numb_boxes']==show_only_with_this_many_GT_boxes,'id'].tolist()\n",
    "fls =[OUTPUT_IMAGES_DIR+l+'.png'  for l in fls]\n",
    "\n",
    "#or just iterate over entire directory of interest\n",
    "# fls=uteda.get_files(OUTPUT_IMAGES_DIR)\n",
    "\n",
    "imgs=[]\n",
    "for fle in fls:\n",
    "    img = Image.open(fle)\n",
    "    img=np.array(img)\n",
    "    imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uteda.plot_imgs(imgs, cols=3, size=30, is_rgb=True, title=\"\", cmap='gray', img_size=(600,600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4a9ffdef130dccae6ae0228c0775bd6719239fc229dcdf8fb568e587357106f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "335px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "145px",
    "left": "-1.09094px",
    "right": "20px",
    "top": "543px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
